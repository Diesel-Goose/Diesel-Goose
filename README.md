# ğŸ¦† Diesel-Goose

**Local-first AI agent framework with persistent memory.**

Built for the Chairman. Designed for billion-scale execution. Faith-aligned, family-first, radically delegated.

---

## What Diesel-Goose Is

Diesel-Goose is a sovereign AI system that runs entirely on your local Mac Mini M4 (or any ARM64 machine). It combines:

- **Local LLM inference** via Ollama (no API costs, no data leakage)
- **Persistent memory** with confidence scoring and privacy controls
- **Structured prompting** with automatic context retrieval
- **Ethical delegation** framework aligned with founder principles

All data stays local. All memories are private. All execution is delegated.

---

## Architecture

```
User Input
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BRAIN (prompt_builder.py)          â”‚
â”‚  â€¢ Retrieves relevant memories      â”‚
â”‚  â€¢ Constructs context-aware prompt  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ollama (local LLM)                 â”‚
â”‚  â€¢ Inference on localhost:11434     â”‚
â”‚  â€¢ Zero external data transmission  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MEMORY (memory_filter.py)          â”‚
â”‚  â€¢ Extracts long-term memories      â”‚
â”‚  â€¢ Scores confidence (0.0-1.0)      â”‚
â”‚  â€¢ Filters for privacy level        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MEMORY (memory_engine.py)          â”‚
â”‚  â€¢ Stores in local JSON (gitignored)â”‚
â”‚  â€¢ Prevents duplicates              â”‚
â”‚  â€¢ Enforces confidence threshold    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Installation

### Prerequisites

- macOS (ARM64 optimized for M4)
- Homebrew
- Python 3.10+

### Step 1: Install Ollama

```bash
# Using Homebrew (recommended)
brew install ollama
brew services start ollama

# Or official installer
curl -fsSL https://ollama.com/install.sh | sh
```

### Step 2: Pull LLM Model

```bash
ollama pull llama3
```

### Step 3: Install Diesel-Goose

```bash
git clone https://github.com/Diesel-Goose/Diesel-Goose.git
cd Diesel-Goose

# Install Python dependencies
pip3 install requests --user --break-system-packages
```

---

## Running

### Interactive Mode (Recommended)

```bash
python3 main.py
```

Type commands naturally. The system will:
1. Retrieve relevant memories
2. Build context-aware prompt
3. Query local Ollama instance
4. Extract and store new memories

### Single Command

```bash
python3 main.py "What's my next priority?"
```

### Run Tests

```bash
python3 main.py --test
```

---

## Commands

| Command | Description |
|---------|-------------|
| `help` | Show available commands |
| `stats` | Display memory statistics |
| `exit` / `quit` / `q` | Exit the program |

---

## Project Structure

```
Diesel-Goose/
â”œâ”€â”€ main.py                 # Entry point
â”œâ”€â”€ README.md              # This file
â”œâ”€â”€ CONTRIBUTING.md        # Contribution guidelines
â”œâ”€â”€ .gitignore            # Protects local memory
â”‚
â”œâ”€â”€ BRAIN/                # Core intelligence
â”‚   â”œâ”€â”€ prompt_builder.py # Prompt construction
â”‚   â”œâ”€â”€ orchestrator.py   # Task orchestration
â”‚   â””â”€â”€ local_llm_agent.py # Ollama interface
â”‚
â”œâ”€â”€ MEMORY/               # Memory system
â”‚   â”œâ”€â”€ memory_engine.py  # Storage engine
â”‚   â”œâ”€â”€ memory_filter.py  # Extraction & scoring
â”‚   â””â”€â”€ memory_store/     # Local data (gitignored)
â”‚
â”œâ”€â”€ AGENTS/               # Agent implementations
â”‚   â”œâ”€â”€ base_agent.py     # Agent base class
â”‚   â””â”€â”€ tests/            # Test suite
â”‚
â””â”€â”€ [Legacy files...]
```

---

## Security Note

**Memory is local-only and gitignored.**

Your conversation history, extracted memories, and personal data are:
- âœ… Stored only in `MEMORY/memory_store/` 
- âœ… Never committed to GitHub
- âœ… Never transmitted to external APIs
- âœ… Protected by `.gitignore` rules

The only external connection is to your local Ollama instance (`localhost:11434`).

---

## Founder Principles

This system embodies the Diesel-Goose operating philosophy:

1. **Radical Delegation** â€“ Delegate once, execute completely
2. **Family First** â€“ Systems exist to protect family time
3. **Faith-Aligned** â€“ Stewardship, integrity, no shortcuts
4. **Local Sovereignty** â€“ Own your data, own your intelligence
5. **Billions or Nothing** â€“ Build for exponential scale

---

## Version

**v2.0** â€“ Local-first architecture with structured memory

Built with â¤ï¸ for the Chairman and his family.

ğŸ¦† **Quack protocol: Active.**
